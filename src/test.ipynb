{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\diogo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Make sure accelerate is installed otherwise the following error may be raised\n",
    "\"\"\"\n",
    "Cannot initialize model with low cpu memory usage because `accelerate` was not found in the environment. \n",
    "Defaulting to `low_cpu_mem_usage=False`. \n",
    "It is strongly recommended to install `accelerate` for faster and less memory-intense model loading. \n",
    "You can do so with: \n",
    "    ```\n",
    "    pip install accelerate\n",
    "    ```\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import gc\n",
    "import diffusers\n",
    "import schedulers\n",
    "import inspect\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from dataclasses import dataclass\n",
    "from tqdm.auto import tqdm\n",
    "from math import ceil\n",
    "\n",
    "import torch\n",
    "from torchvision import transforms as tfms\n",
    "\n",
    "from autoencoders import AutoEncoder, VAE\n",
    "\n",
    "from diffusers import AutoencoderKL, LMSDiscreteScheduler, UNet2DConditionModel\n",
    "from transformers import CLIPTextModel, CLIPTokenizer, logging\n",
    "from huggingface_hub import notebook_login\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "\n",
    "# TODO this will have to be changed for the main.py code\n",
    "if not (Path.home()/'.cache/huggingface'/'token').exists(): notebook_login()\n",
    "\n",
    "# Supress CLIPTextModel warnings\n",
    "logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StableDiffusionPipeline():\n",
    "\n",
    "    def __init__(self, prompt, inference_steps, guidance_scale,\n",
    "                 height=512, width=512,\n",
    "                 scheduler=\"LMSDiscreteScheduler\", scheduler_params={},\n",
    "                 seeded_generator=torch.manual_seed(32)):\n",
    "        self.prompt = prompt\n",
    "\n",
    "        self.inference_steps = inference_steps\n",
    "        self.guidance_scale = guidance_scale\n",
    "\n",
    "        self.height = height\n",
    "        self.width = width\n",
    "\n",
    "        self.seeded_generator = seeded_generator\n",
    "\n",
    "        # Device\n",
    "        self.initialise_device()\n",
    "\n",
    "        # Components\n",
    "        self.initialise_components(scheduler, scheduler_params)\n",
    "        self.components_to_gpu()\n",
    "\n",
    "        # Initialise embeddings, scheduler and latents that are required prior to inference\n",
    "        self.initialise_text_embeddings(self.prompt)\n",
    "        self.initialise_scheduler()\n",
    "        self.initialise_latents()\n",
    "\n",
    "    def initialise_device(self):\n",
    "        # Set device based on CUDA, MPS (MacOS), CPU\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "\n",
    "        if \"mps\" == self.device: \n",
    "            os.environ['PYTORCH_ENABLE_MPS_FALLBACK'] = \"1\"\n",
    "\n",
    "    def initialise_components(self, scheduler, scheduler_params):\n",
    "        # In the paper our q() - the distribution over a noisey object at time step t has a closed form solution\n",
    "        # \n",
    "        self.scheduler = self.select_scheduler(scheduler, scheduler_params)\n",
    "        \n",
    "        # Load the autoencoder model which will be used to decode the latents into image space.\n",
    "        self.vae = AutoencoderKL.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"vae\")\n",
    "\n",
    "        # # Loading a previously trained model using autoencoders.py\n",
    "        # state_dict = torch.load(\"../training/saved_models/vae.pth\")\n",
    "        # # Load the state_dict into the model\n",
    "        # self.vae = VAE()\n",
    "        # self.vae.load_state_dict(state_dict)\n",
    "        # # Make sure that we set the model to evaluation mode to make sure that the running statistics that were calculated in batch normalisation\n",
    "        # # are consistent during inference\n",
    "        # self.vae.eval()\n",
    "\n",
    "        # The UNet model for generating the latents.\n",
    "        self.unet = UNet2DConditionModel.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"unet\")\n",
    "        \n",
    "        # Load the tokeniser and text encoder to tokenize and encode the text.\n",
    "        # CLIP uses a ViT like transformer that gets visual features and a causal language model for the text features\n",
    "        # Both the text and visual features are proejcted into a latent space with  identical dimensions and a dot product between the projected image and the text features is used as a similarity score\n",
    "        # Images are split into fixed-size and non overlapping sequendes called patches - positional embeddings are also added.\n",
    "        # If needed images can be resized and normalised using the CLIPImageProcessor\n",
    "        # The CLIPTokeniser encodes the text\n",
    "        self.tokeniser = CLIPTokenizer.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "        self.text_encoder = CLIPTextModel.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "\n",
    "\n",
    "    def select_scheduler(self, scheduler_type: str, scheduler_params: dict=None):\n",
    "        # Try to get the scheduler from the diffusers library\n",
    "        try:\n",
    "            SchedulerClass = getattr(diffusers, scheduler_type)\n",
    "        except AttributeError:\n",
    "            try:\n",
    "                # If it is not in the diffusers library then we can check our schedulers module that uses the SchedulerMixin\n",
    "                SchedulerClass = getattr(schedulers, scheduler_type)\n",
    "            except AttributeError:\n",
    "                print(self.get_custom_schedulers())\n",
    "                raise AttributeError(f\"The following scheduler {scheduler_type} does not exist in the diffusers library or in the scheduler file.\\n The following are possible choices in the diffusers library:\\n{self.get_diffusers_schedulers()}\")\n",
    "\n",
    "\n",
    "        self.validate_scheduler_params(SchedulerClass, scheduler_params)\n",
    "        # Instantiate the scheduler with the provided parameters - make sure that your parameters that you pass in match the requirements\n",
    "        return SchedulerClass(**scheduler_params)\n",
    "\n",
    "    def validate_scheduler_params(self, scheduler_class, scheduler_params):\n",
    "        # Get the __init__ method of the scheduler class\n",
    "        init_signature = inspect.signature(scheduler_class.__init__)\n",
    "        \n",
    "        # Get the parameter names (excluding 'self')\n",
    "        valid_params = [param for param in init_signature.parameters if param != 'self']\n",
    "\n",
    "        # Identify invalid parameters\n",
    "        invalid_params = [param for param in scheduler_params if param not in valid_params]\n",
    "\n",
    "        if invalid_params:\n",
    "            print(f\"Error: Invalid parameters {invalid_params} for {scheduler_class.__name__}.\")\n",
    "            print(f\"Expected parameters are: {valid_params}\")\n",
    "            return False\n",
    "        return True\n",
    "\n",
    "    def get_diffusers_schedulers(self):\n",
    "        # We can use the inspect module if we get all the members of the diffusers module and we check if it ends with Scheduler\n",
    "        return [name for name, obj in inspect.getmembers(diffusers) if inspect.isclass(obj) and name.endswith('DiscreteScheduler')]\n",
    "    \n",
    "    def get_custom_schedulers(self):\n",
    "        # We can use the inspect module if we get all the members of the diffusers module and we check if it ends with Scheduler\n",
    "        return [name for name, obj in inspect.getmembers(schedulers) if inspect.isclass(obj) and name.endswith('Scheduler')]\n",
    "\n",
    "    def components_to_gpu(self):\n",
    "        # Move the initialised components to the gpu\n",
    "        self.vae = self.vae.to(self.device)\n",
    "        self.text_encoder = self.text_encoder.to(self.device)\n",
    "        self.unet = self.unet.to(self.device)\n",
    "\n",
    "    def initialise_text_embeddings(self, prompt):\n",
    "        # Conditioned text embeddings\n",
    "        self.prompt_tokens = self.tokeniser(prompt, padding=\"max_length\", max_length=self.tokeniser.model_max_length, truncation=True, return_tensors=\"pt\")\n",
    "        with torch.no_grad():\n",
    "            # text_embeddings = self.text_encoder(text_input.input_ids.to(self.device))[0]\n",
    "            text_embeddings = self.get_embeddings(self.prompt_tokens.input_ids.to(self.device))\n",
    "        \n",
    "        # Matching the conditioned max max length and creating the unconditioned embeddings\n",
    "        max_length = self.prompt_tokens.input_ids.shape[-1]\n",
    "        uncond_input = self.tokeniser([\"\"], padding=\"max_length\", max_length=max_length, return_tensors=\"pt\")\n",
    "        with torch.no_grad():\n",
    "            uncond_embeddings = self.text_encoder(uncond_input.input_ids.to(self.device))[0]\n",
    "\n",
    "        # Concatenate both\n",
    "        self.text_embeddings = torch.cat([uncond_embeddings, text_embeddings])\n",
    "\n",
    "    def visualise_tokenisation(self, show_tensor=True, show_attention_mask=True):\n",
    "        # Starts off with a prompt - then we tokenise it into a series of tokens using our CLIPTokeniser\n",
    "        # This tokeniser makes sure it is padded\n",
    "        printed_tokens = set()\n",
    "\n",
    "        if show_tensor or show_attention_mask:\n",
    "            if show_tensor:\n",
    "                print(f\"Tokenised text tensor:\\n{self.prompt_tokens['input_ids'][0]}\\n\")\n",
    "            if show_attention_mask:\n",
    "                print(f\"Tokenised text tensor:\\n{self.prompt_tokens['attention_mask'][0]}\\n\")\n",
    "        else:\n",
    "            print(\"For the sake of clarity the padded tokens and the attention mask are removed from the output, run this method with show_tensor=True and/or attention_mask=True to see them respectively.\\n\")\n",
    "\n",
    "        # See the individual tokens\n",
    "        for token in self.prompt_tokens['input_ids'][0]:\n",
    "            token_value = int(token)\n",
    "            if token_value not in printed_tokens:\n",
    "                print(f\"Token: {token} - Value: {self.tokeniser.decoder.get(token_value)}\")\n",
    "                printed_tokens.add(token_value)\n",
    "\n",
    "    def build_causal_attention_mask(self, bsz, seq_len, dtype):\n",
    "        # lazily create causal attention mask, with full attention between the vision tokens\n",
    "        # pytorch uses additive attention mask; fill with -inf\n",
    "        mask = torch.empty(bsz, seq_len, seq_len, dtype=dtype)\n",
    "        mask.fill_(torch.tensor(torch.finfo(dtype).min))\n",
    "        mask.triu_(1)  # zero out the lower diagonal\n",
    "        mask = mask.unsqueeze(1)  # expand mask\n",
    "        return mask\n",
    "\n",
    "    def get_embeddings(self, input_embeddings):\n",
    "        # TODO undestand code and comment it\n",
    "        token_embeddings = self.text_encoder.text_model.embeddings.token_embedding(input_embeddings)\n",
    "        # TODO why only 77\n",
    "        position_ids = self.text_encoder.text_model.embeddings.position_ids[:, :77]\n",
    "        positional_embeddings = self.text_encoder.text_model.embeddings.position_embedding(position_ids)\n",
    "\n",
    "        embedding = token_embeddings + positional_embeddings\n",
    "\n",
    "        # CLIP's text model uses causal mask, so we prepare it here:\n",
    "        bsz, seq_len = embedding.shape[:2]\n",
    "        causal_attention_mask = self.build_causal_attention_mask(bsz, seq_len, embedding.dtype)\n",
    "\n",
    "        # Getting the output embeddings involves calling the model with passing output_hidden_states=True\n",
    "        # so that it doesn't just return the pooled final predictions:\n",
    "        encoder_outputs = self.text_encoder.text_model.encoder(\n",
    "            inputs_embeds=embedding,\n",
    "            attention_mask=None,\n",
    "            causal_attention_mask=causal_attention_mask.to(self.device),\n",
    "            output_attentions=None,\n",
    "            output_hidden_states=True,\n",
    "            return_dict=None,\n",
    "        )\n",
    "\n",
    "        # We're interested in the output hidden state only\n",
    "        output = encoder_outputs[0]\n",
    "\n",
    "        # There is a final layer norm we need to pass these through\n",
    "        output = self.text_encoder.text_model.final_layer_norm(output)\n",
    "        return output\n",
    "\n",
    "    def initialise_scheduler(self):\n",
    "        # The timesteps - betas that we define are values that are between 0 and 1 and change how the distribution is changed\n",
    "        # When we initially start with a beta value of 0 it starts off with the image that we started with and 0 noise applied\n",
    "        # It makes sense to scale the variance (the amount of noise that we add) however we must make sure that we also vary the \n",
    "        # mean of the image being added - we have to make sure that as the variance grows we scale the mean down by \n",
    "        # roo(1 - beta) so that our distribution remains and our pure white noise at the end of our sampling is a standard normal distribution\n",
    "        self.scheduler.set_timesteps(self.inference_steps)\n",
    "        self.scheduler.timesteps = self.scheduler.timesteps.to(torch.float32)\n",
    "\n",
    "    def visualise_scheduler(self, scheduler=None, timesteps=None):\n",
    "        # TODO figure out how scheduler.scale_model_input works and what it does (what scaling / pre conditioning is)\n",
    "        if not scheduler:\n",
    "            scheduler = self.scheduler\n",
    "\n",
    "        if timesteps:\n",
    "            scheduler.set_timesteps(timesteps)\n",
    "\n",
    "        # Visualising the sigmas (the amount of noise) that the scheduler will choose to add to our latents\n",
    "        plt.plot(scheduler.sigmas)\n",
    "        plt.title('Noise Schedule')\n",
    "        plt.xlabel('Steps')\n",
    "        plt.ylabel('Sigma')\n",
    "        plt.show()\n",
    "\n",
    "    def visualise_timestep_embeddings(self):\n",
    "        _, ax = plt.subplots(1, 1, figsize=(8,6))\n",
    "        sns.heatmap(self.scheduler.timesteps, cmap='bwr', ax=ax)\n",
    "        ax.set_xlabel('Embedding Dimension')\n",
    "        ax.set_ylabel('Timestep')\n",
    "        plt.show()\n",
    "\n",
    "    def visualise_noised_latents(self, latents, scheduler, timesteps):\n",
    "        max_timesteps = len(scheduler.timesteps)\n",
    "        assert timesteps < max_timesteps, \"The timestep chosen to visualise the noised image must be lower than the current max {max_timesteps}\"\n",
    "\n",
    "        noise = torch.rand_like(latents)\n",
    "        # This add noise function is the same noised_sample = original + noise * sigmas\n",
    "        encoded_and_noised = scheduler.add_noise(latents, noise, timesteps=torch.tensor([scheduler.timesteps[timesteps]]))\n",
    "        decoded_latents = self.decode_latents(encoded_and_noised.float().to(\"cuda\"))\n",
    "        plt.imshow(self.scale_image(decoded_latents))\n",
    "\n",
    "    def initialise_latents(self):\n",
    "        # Prep latents - our latents are 64x smaller so we generate the noise in the latent dimension\n",
    "        # We can generate an object by sampling white noise from a standard normal distribution adn then iteratively sampling x_t-1\n",
    "        self.latents = torch.randn(\n",
    "            (1, self.unet.config.in_channels, self.height // 8, self.width // 8),\n",
    "            generator=self.seeded_generator,\n",
    "        )\n",
    "        self.latents = self.latents.to(self.device)\n",
    "        self.latents = self.latents * self.scheduler.init_noise_sigma # Scaling (previous versions did latents = latents * self.scheduler.sigmas[0]\n",
    "\n",
    "    def update_prompt(self, prompt):\n",
    "        self.prompt = prompt\n",
    "        self.initialise_text_embeddings(self.prompt)\n",
    "\n",
    "    def image_to_latents(self, image):\n",
    "        # Output the latents of a single image\n",
    "        with torch.no_grad():\n",
    "            # TODO break down this scaling into separate components and explain it\n",
    "            latent = self.vae.encode(tfms.ToTensor()(image).unsqueeze(0).to(self.device)*2-1) # Note scaling\n",
    "        images = 0.18215 * latent.latent_dist.sample()\n",
    "        images = images.to(\"cpu\")\n",
    "        self.display_images(np.array(images[0]), 4, cmap=\"gray\")\n",
    "        return images\n",
    "\n",
    "    def decode_latents(self, latents):\n",
    "        # TODO figure out why these are scaled scale and decode the image latents with vae\n",
    "        print(latents.shape)\n",
    "        latents = 1 / 0.18215 * latents\n",
    "        print(latents.shape)\n",
    "        with torch.no_grad():\n",
    "            return self.vae.decode(latents[0]).sample\n",
    "\n",
    "    def scale_image(self, image):\n",
    "        # TODO understand and comment why this is done\n",
    "        image = (image / 2 + 0.5).clamp(0, 1)\n",
    "        image = image.detach().cpu().permute(0, 2, 3, 1).numpy()\n",
    "        image = (image * 255).round().astype(\"uint8\")\n",
    "        return Image.fromarray(image[0])\n",
    "\n",
    "    def show_generated_image(self):\n",
    "        plt.imshow(self.image)\n",
    "\n",
    "    def display_images(self, images, cols=1, **kwargs):\n",
    "        num_images = len(images)\n",
    "        rows = ceil(num_images / cols)\n",
    "\n",
    "        # Create subplots and flatten it so that we get an easy iterable\n",
    "        _, axs = plt.subplots(rows, cols, figsize=(15, 5*rows))\n",
    "        axs = axs.flatten()\n",
    "        \n",
    "        # Iterate through the images and plot them - making sure to turn off the axis\n",
    "        for i, image in enumerate(images):\n",
    "            axs[i].imshow(image, **kwargs)\n",
    "            axs[i].axis('off')\n",
    "\n",
    "        # We can use the remainder of the i variable to hide any unused subplots\n",
    "        for j in range(i + 1, len(axs)):\n",
    "            axs[j].axis('off')\n",
    "\n",
    "        # Adjust layout to prevent overlapping\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    def generate(self, show=True):\n",
    "        # with autocast(\"cuda\"):\n",
    "        for _, t in tqdm(enumerate(self.scheduler.timesteps), total=len(self.scheduler.timesteps)):\n",
    "            # expand the latents if we are doing classifier-free guidance to avoid doing two forward passes.\n",
    "            latent_model_input = torch.cat([self.latents] * 2)\n",
    "\n",
    "            # By preconditioning or scaling the latents we are ensuring that we are improving the stability and performance of the model by compensating for the different noise levels\n",
    "            # and adjusting the variability of the magnitude of the inputs as the sigma will always be chosen by the scheduler\n",
    "            latent_model_input = self.scheduler.scale_model_input(latent_model_input, t)\n",
    "\n",
    "            # predict the noise residual\n",
    "            with torch.no_grad():\n",
    "                # The encoder hidden states in the unet is the encoded text embeddings that CLIP provides that will guide the model\n",
    "                noise_pred = self.unet(latent_model_input, t, encoder_hidden_states=self.text_embeddings).sample\n",
    "\n",
    "            # Guidance - we initially combined the two text embeddings and concatenated two latents to do the conditioned and unconditioned together\n",
    "            # Use .chunk to split the noise prediction of the two concatenated tensors into the unconditioned noise prediction\n",
    "            # and the conditioned on text noise prediction, these are then updated using the guidance scale formula\n",
    "            # Take the difference between the conditioned and the unconditioned noise - the higher the guidance scale the more we follow the conditioned noise\n",
    "            noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n",
    "            noise_pred = noise_pred_uncond + self.guidance_scale * (noise_pred_text - noise_pred_uncond)\n",
    "\n",
    "            # Here we are essentially sampling from the posterior distribution in order to remove the noise\n",
    "            self.latents = self.scheduler.step(noise_pred, t, self.latents).prev_sample\n",
    "\n",
    "        self.image = self.decode_latents(self.latents)\n",
    "        self.image = self.scale_image(self.image)\n",
    "        if show == True:\n",
    "            self.show_generated_image()\n",
    "\n",
    "        # self.clear_cuda()\n",
    "\n",
    "        return self.image\n",
    "\n",
    "    def clear_cuda(self):\n",
    "        # Clearing CUDA so that it does not run out of memory during multiple instantiations - big problem especially in notebooks\n",
    "        torch.cuda.synchronize()\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/40 [00:00<?, ?it/s]c:\\Users\\diogo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\diffusers\\models\\attention_processor.py:2358: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  hidden_states = F.scaled_dot_product_attention(\n",
      "  0%|          | 0/40 [00:08<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 41\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# Generating an image and the visualising its latents\u001b[39;00m\n\u001b[0;32m     40\u001b[0m pipeline \u001b[38;5;241m=\u001b[39m StableDiffusionPipeline(prompt, inference_steps, guidance_scale, scheduler\u001b[38;5;241m=\u001b[39mscheduler, scheduler_params\u001b[38;5;241m=\u001b[39m{})\n\u001b[1;32m---> 41\u001b[0m image \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     43\u001b[0m latents \u001b[38;5;241m=\u001b[39m pipeline\u001b[38;5;241m.\u001b[39mimage_to_latents(image)\n\u001b[0;32m     44\u001b[0m pipeline\u001b[38;5;241m.\u001b[39mvisualise_noised_latents(latents, pipeline\u001b[38;5;241m.\u001b[39mscheduler, \u001b[38;5;241m5\u001b[39m)\n",
      "Cell \u001b[1;32mIn[2], line 309\u001b[0m, in \u001b[0;36mStableDiffusionPipeline.generate\u001b[1;34m(self, show)\u001b[0m\n\u001b[0;32m    306\u001b[0m \u001b[38;5;66;03m# predict the noise residual\u001b[39;00m\n\u001b[0;32m    307\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m    308\u001b[0m     \u001b[38;5;66;03m# The encoder hidden states in the unet is the encoded text embeddings that CLIP provides that will guide the model\u001b[39;00m\n\u001b[1;32m--> 309\u001b[0m     noise_pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlatent_model_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext_embeddings\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msample\n\u001b[0;32m    311\u001b[0m \u001b[38;5;66;03m# Guidance - we initially combined the two text embeddings and concatenated two latents to do the conditioned and unconditioned together\u001b[39;00m\n\u001b[0;32m    312\u001b[0m \u001b[38;5;66;03m# Use .chunk to split the noise prediction of the two concatenated tensors into the unconditioned noise prediction\u001b[39;00m\n\u001b[0;32m    313\u001b[0m \u001b[38;5;66;03m# and the conditioned on text noise prediction, these are then updated using the guidance scale formula\u001b[39;00m\n\u001b[0;32m    314\u001b[0m \u001b[38;5;66;03m# Take the difference between the conditioned and the unconditioned noise - the higher the guidance scale the more we follow the conditioned noise\u001b[39;00m\n\u001b[0;32m    315\u001b[0m noise_pred_uncond, noise_pred_text \u001b[38;5;241m=\u001b[39m noise_pred\u001b[38;5;241m.\u001b[39mchunk(\u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\diogo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\diogo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\diogo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\diffusers\\models\\unets\\unet_2d_condition.py:1246\u001b[0m, in \u001b[0;36mUNet2DConditionModel.forward\u001b[1;34m(self, sample, timestep, encoder_hidden_states, class_labels, timestep_cond, attention_mask, cross_attention_kwargs, added_cond_kwargs, down_block_additional_residuals, mid_block_additional_residual, down_intrablock_additional_residuals, encoder_attention_mask, return_dict)\u001b[0m\n\u001b[0;32m   1244\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmid_block \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1245\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmid_block, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_cross_attention\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmid_block\u001b[38;5;241m.\u001b[39mhas_cross_attention:\n\u001b[1;32m-> 1246\u001b[0m         sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmid_block\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1247\u001b[0m \u001b[43m            \u001b[49m\u001b[43msample\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1248\u001b[0m \u001b[43m            \u001b[49m\u001b[43memb\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1249\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1250\u001b[0m \u001b[43m            \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1251\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcross_attention_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attention_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1252\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1253\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1254\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1255\u001b[0m         sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmid_block(sample, emb)\n",
      "File \u001b[1;32mc:\\Users\\diogo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\diogo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\diogo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\diffusers\\models\\unets\\unet_2d_blocks.py:889\u001b[0m, in \u001b[0;36mUNetMidBlock2DCrossAttn.forward\u001b[1;34m(self, hidden_states, temb, encoder_hidden_states, attention_mask, cross_attention_kwargs, encoder_attention_mask)\u001b[0m\n\u001b[0;32m    882\u001b[0m         hidden_states \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mcheckpoint\u001b[38;5;241m.\u001b[39mcheckpoint(\n\u001b[0;32m    883\u001b[0m             create_custom_forward(resnet),\n\u001b[0;32m    884\u001b[0m             hidden_states,\n\u001b[0;32m    885\u001b[0m             temb,\n\u001b[0;32m    886\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mckpt_kwargs,\n\u001b[0;32m    887\u001b[0m         )\n\u001b[0;32m    888\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 889\u001b[0m         hidden_states \u001b[38;5;241m=\u001b[39m \u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    890\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    891\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    892\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcross_attention_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attention_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    893\u001b[0m \u001b[43m            \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    894\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    895\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    896\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    897\u001b[0m         hidden_states \u001b[38;5;241m=\u001b[39m resnet(hidden_states, temb)\n\u001b[0;32m    899\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
      "File \u001b[1;32mc:\\Users\\diogo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\diogo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\diogo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\diffusers\\models\\transformers\\transformer_2d.py:407\u001b[0m, in \u001b[0;36mTransformer2DModel.forward\u001b[1;34m(self, hidden_states, encoder_hidden_states, timestep, added_cond_kwargs, class_labels, cross_attention_kwargs, attention_mask, encoder_attention_mask, return_dict)\u001b[0m\n\u001b[0;32m    405\u001b[0m     batch_size, _, height, width \u001b[38;5;241m=\u001b[39m hidden_states\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m    406\u001b[0m     residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[1;32m--> 407\u001b[0m     hidden_states, inner_dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_operate_on_continuous_inputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    408\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_input_vectorized:\n\u001b[0;32m    409\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlatent_image_embedding(hidden_states)\n",
      "File \u001b[1;32mc:\\Users\\diogo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\diffusers\\models\\transformers\\transformer_2d.py:484\u001b[0m, in \u001b[0;36mTransformer2DModel._operate_on_continuous_inputs\u001b[1;34m(self, hidden_states)\u001b[0m\n\u001b[0;32m    481\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm(hidden_states)\n\u001b[0;32m    483\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_linear_projection:\n\u001b[1;32m--> 484\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproj_in\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    485\u001b[0m     inner_dim \u001b[38;5;241m=\u001b[39m hidden_states\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    486\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m hidden_states\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mreshape(batch, height \u001b[38;5;241m*\u001b[39m width, inner_dim)\n",
      "File \u001b[1;32mc:\\Users\\diogo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\diogo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\diogo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:458\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    457\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 458\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\diogo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:454\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    450\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    451\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[0;32m    452\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[0;32m    453\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[1;32m--> 454\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    455\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# TODO create a config like GPT\n",
    "@dataclass\n",
    "class SDPipelineConfig:\n",
    "    \"\"\"\n",
    "    Stable Diffusion Pipeline Config\n",
    "\n",
    "    prompt: The text prompt you want to condition the image generation on\n",
    "    height: Height of image generated                            \n",
    "    width: Width of image generated                              \n",
    "    inference_steps: Number of diffusion steps that you want to take place                   \n",
    "    guidance_scale: Guidance value that affects how closely the prompt affects diffusion                        \n",
    "    seed: Random generation seed to keep results reproducible\n",
    "    \"\"\"\n",
    "    prompt: str = \"A parrot\"\n",
    "    height: int = 512                              \n",
    "    width: int = 512                                 \n",
    "    inference_steps: int = 15                         \n",
    "    guidance_scale: float = 7.5                        \n",
    "    seed: int = 42\n",
    "\n",
    "    # TODO see if any assertions can be added here to make sure the values are good e.g height and width being a factor of 8 (explain why too)\n",
    "\n",
    "prompt = \"A parrot\"\n",
    "height = 512                               \n",
    "width = 512                                 \n",
    "inference_steps = 40                       \n",
    "guidance_scale = 7.5 \n",
    "seeded_generator = torch.manual_seed(65)\n",
    "\n",
    "# ['EulerAncestralDiscreteScheduler', 'EulerDiscreteScheduler', 'HeunDiscreteScheduler', 'KDPM2AncestralDiscreteScheduler', 'KDPM2DiscreteScheduler', 'LMSDiscreteScheduler']\n",
    "scheduler = \"LMSDiscreteScheduler\"\n",
    "scheduler_params = {\n",
    "    \"beta_start\": 0.00085,\n",
    "    \"beta_end\": 0.012,\n",
    "    \"beta_schedule\": \"scaled_linear\",\n",
    "    \"num_train_timesteps\": 1000\n",
    "}\n",
    "\n",
    "# Generating an image and the visualising its latents\n",
    "pipeline = StableDiffusionPipeline(prompt, inference_steps, guidance_scale, scheduler=scheduler, scheduler_params={})\n",
    "image = pipeline.generate(show=True)\n",
    "\n",
    "latents = pipeline.image_to_latents(image)\n",
    "pipeline.visualise_noised_latents(latents, pipeline.scheduler, 5)\n",
    "pipeline.visualise_tokenisation(show_tensor=True, show_attention_mask=True)\n",
    "pipeline.visualise_scheduler()\n",
    "pipeline.get_diffusers_schedulers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([999.0000, 988.9091, 978.8182, 968.7273, 958.6364, 948.5455, 938.4545,\n",
       "        928.3636, 918.2727, 908.1818, 898.0909, 888.0000, 877.9091, 867.8182,\n",
       "        857.7273, 847.6364, 837.5455, 827.4545, 817.3636, 807.2727, 797.1818,\n",
       "        787.0909, 777.0000, 766.9091, 756.8182, 746.7273, 736.6364, 726.5455,\n",
       "        716.4545, 706.3636, 696.2727, 686.1818, 676.0909, 666.0000, 655.9091,\n",
       "        645.8182, 635.7273, 625.6364, 615.5455, 605.4545, 595.3636, 585.2727,\n",
       "        575.1818, 565.0909, 555.0000, 544.9091, 534.8182, 524.7273, 514.6364,\n",
       "        504.5454, 494.4546, 484.3636, 474.2727, 464.1818, 454.0909, 444.0000,\n",
       "        433.9091, 423.8182, 413.7273, 403.6364, 393.5454, 383.4546, 373.3636,\n",
       "        363.2727, 353.1818, 343.0909, 333.0000, 322.9091, 312.8182, 302.7273,\n",
       "        292.6364, 282.5454, 272.4546, 262.3636, 252.2727, 242.1818, 232.0909,\n",
       "        222.0000, 211.9091, 201.8182, 191.7273, 181.6364, 171.5455, 161.4545,\n",
       "        151.3636, 141.2727, 131.1818, 121.0909, 111.0000, 100.9091,  90.8182,\n",
       "         80.7273,  70.6364,  60.5455,  50.4545,  40.3636,  30.2727,  20.1818,\n",
       "         10.0909,   0.0000])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.scheduler.timesteps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO add this function as a callback that can be triggered whenever - check if there are any other callbacks that can be used\n",
    "\n",
    "def show_gpu_usage():\n",
    "    if torch.cuda.is_available():\n",
    "        megabyte = 1024 ** 2\n",
    "        gpu_memory_allocated = torch.cuda.memory_allocated(0) / megabyte\n",
    "        gpu_memory_reserved = torch.cuda.memory_reserved(0) / megabyte\n",
    "        print(f\"Memory Allocated: {gpu_memory_allocated:.2f} MB\")\n",
    "        print(f\"Memory Reserved: {gpu_memory_reserved:.2f} MB\")\n",
    "    else:\n",
    "        print(\"CUDA is not currently in use.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO split into a visualiser class, that has all the visualisation functions\n",
    "# TODO selection of the custom networks along with training the\n",
    "\n",
    "# TODO add to the pipeline but change format and colour - comment code and understand it\n",
    "    # # Plot some test embeddings\n",
    "    # test_time_embeddings = get_timestep_embedding(\n",
    "    #     torch.arange(0,1000), embedding_dim=60\n",
    "    # )\n",
    "    # fig, ax = plt.subplots(1,1,figsize=(6,5))\n",
    "    # sns.heatmap(test_time_embeddings, cmap='bwr', ax=ax)\n",
    "    # ax.set_xlabel('Embedding Dimension')\n",
    "    # ax.set_ylabel('Timestep')\n",
    "    # plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
