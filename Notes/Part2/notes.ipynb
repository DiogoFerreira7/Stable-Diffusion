{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spherical linear interpolation\n",
    "\n",
    "# SLERP between two latent noise starting points, you can start from different prompts and see how it moves through the latent space.\n",
    "# E.g you can start with a dinosaur and slowly turn it into a bird.\n",
    "\n",
    "# CLIP takes the alt tags of images and then takes two models\n",
    "# Image encodes and text encoders into 2 vectors - the loss function of the text encoding should be as close to the txt encoding as possible\n",
    "# The CL (contrastive loss) specifies that we should maximise the encodings that match but minimse the others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CFGD classifier free guidance\n",
    "# We can add our model embedding into our unet and an empty image of just purely random noise into our unet too.\n",
    "# For different numbers of guided diffusion this is quite awkward.\n",
    "# If we pass this guidance along with the noise and caption to the student it learns how the guidance also affects the teacher model too along steps.\n",
    "\n",
    "# Progressive distillation for fast samplng of diffusion models.\n",
    "\n",
    "# unconditional + (guidance * (text - unconditional))\n",
    "# The result gives quite a big value, the eventual update of our diff will be extremely large so what we can do is scale it\n",
    "# This is a rescaled guidance update \n",
    "\n",
    "# You can also use a dynamic cosine scheduler like guidance scale, this means that it would start off with a direction that has been guided and then it will introduce its own randomness into the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following research papers managed to bring down the number of steps needed to be done\n",
    "# Distillation of guided diffusion models\n",
    "# Progressive distillation\n",
    "\n",
    "# Distillation - take a teacher network that is slow and big, the student network tries to do the same thing faster or with less memory usage\n",
    "# By training a new model that takes as input the noisy image into a new unet and we compare the result to the image a magnitude of steps further\n",
    "# The UNet will then learn to take these incomplete images and turn them into much faster processes\n",
    "\n",
    "# Teacher models are a fully trained stable diffusion model\n",
    "# We want to train our new Unet to go from 1 step to learn 2 steps of noise\n",
    "# Then we take this new student model that predicts 2 steps of noise adn we create a new student model that will have this one as the teacher.\n",
    "# The new student model C that predicts 2 steps of its teacher (studetn model B) will predict 2 per step of its teacher (which already predicts 2)\n",
    "# These will compound an predict much faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLIPTextModel has a pretrained \"open ai clip vit large\" model that we can download\n",
    "# We can also get a CLIPTokeniser that is also already pretrained\n",
    "\n",
    "# Diffusers allows you to also download a VAE and unet\n",
    "# LMSDiscrete Scheduler can define the amount of noise\n",
    "\n",
    "# You have to first tokenise the prompt and the input that is passed will be padded then the attention mask uses zero to represent the tokens that we aer not interested in.\n",
    "# The text encoder will give teh embeddings for the text prompt that we used.\n",
    "# For classifier free guidance we will also need the embeddings to be done for the conditioned and the unconditioned embeddings. In practice they can both be concatenated both into a single batch\n",
    "\n",
    "# We then start from pure gaussian noise and they will be our initial latents. .half() will set it to half precisiion, they will make GPUs much faster.\n",
    "# Timesteps are measures of how much noise is being added if you plot hte noise from the schedulers then you will see the amount of noise added increases the further away\n",
    "\n",
    "#We can predict the noise residual using the unet, we perform guidance by predicting the unconditional and the text\n",
    "# Then our acual prediction is the unconditional + the guidance_scale * (pred_text - pred_unconditional)\n",
    "# The higher the guidance scale the more the prediction will include the specifics of our prompt\n",
    "\n",
    "# We then use our vae to decode the value and we can output our image in PIL\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLIP Interrogators do not return the CLIP prompt but return a good general initial description\n",
    "# If we have an image embedding requires inverting an encoder which cannot be done but can be approximated as many different prompts can map to similar values\n",
    "# CLIP Interrogators - mix and match everything and try to find a good suitable description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
