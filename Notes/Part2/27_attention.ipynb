{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "273a6ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math,torch\n",
    "from torch import nn\n",
    "\n",
    "from miniai.activations import *\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from diffusers.models.attention import AttentionBlock\n",
    "\n",
    "set_seed(42)\n",
    "x = torch.randn(64,32,16,16)\n",
    "\n",
    "# Attention is used in the stable diffusion implementation\n",
    "# Attention lets you take a weighted average of other pixels\n",
    "# 1D attention is what is used in Stable Diffusion\n",
    "# This attention that is 1d will flatten everything down into a 1d output, it will however be a matrix because we will have channels\n",
    "\n",
    "# To do this attention we take a weighted average of thes pixels each pixel is the original pixel + the weighted average - the weights will sum to 1\n",
    "\n",
    "# K, Q, V are all being passed the same matrix - self-attention\n",
    "# Softmax will make the weights get summed to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8174db82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now trying to make the identical stable diffusion attention block\n",
    "\n",
    "# We can flatten thsee out with x.view()\n",
    "# The *x.shape[:2] will unpack these as args into the function since we just want to copy the exact dimension and then -1 will unpack the rest of the tensors\n",
    "# So then we just transpose the first two dimensions\n",
    "t = x.view(*x.shape[:2], -1).transpose(1, 2)\n",
    "t.shape\n",
    "\n",
    "# Batch, Sequence, Dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51245dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 32 different projections\n",
    "ni = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f10cb36",
   "metadata": {},
   "outputs": [],
   "source": [
    "sk = nn.Linear(ni, ni)\n",
    "sq = nn.Linear(ni, ni)\n",
    "sv = nn.Linear(ni, ni)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15237a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass the key query values from the weights\n",
    "k = sk(t)\n",
    "q = sq(t)\n",
    "v = sv(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d34cd0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# THen we transpose the dimensions  and this is exactly self attention we just need to normalise them\n",
    "(q @ k.transpose(1,2)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15062786",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We also have normalisation, group norm will do batch norm split into channels\n",
    "# taking the q k v and then 2d self atetntion we need to noramlise them, transpoe teh dimensions\n",
    "# matrix multiplcation - we have change the scale by multipllying so we need to square root by the numbero f input dimensions\n",
    "# then we do the softmax and apply our projection to map things needed\n",
    "# Then we reshape it back\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, ni):\n",
    "        super().__init__()\n",
    "        self.scale = math.sqrt(ni)\n",
    "        self.norm = nn.GroupNorm(1, ni)\n",
    "        self.q = nn.Linear(ni, ni)\n",
    "        self.k = nn.Linear(ni, ni)\n",
    "        self.v = nn.Linear(ni, ni)\n",
    "        self.proj = nn.Linear(ni, ni)\n",
    "    \n",
    "    # Then we have a residual path in our attention block\n",
    "    def forward(self, x):\n",
    "        inp = x\n",
    "        n,c,h,w = x.shape\n",
    "        x = self.norm(x)\n",
    "        x = x.view(n, c, -1).transpose(1, 2)\n",
    "        q = self.q(x)\n",
    "        k = self.k(x)\n",
    "        v = self.v(x)\n",
    "        s = (q@k.transpose(1,2))/self.scale\n",
    "        x = s.softmax(dim=-1)@v\n",
    "        x = self.proj(x)\n",
    "        x = x.transpose(1,2).reshape(n,c,h,w)\n",
    "        return x + inp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb48706",
   "metadata": {},
   "outputs": [],
   "source": [
    "sa = SelfAttention(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb4ae6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ra = sa(x)\n",
    "ra.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce69830a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ra[0,0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a380fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cp_parms(a,b):\n",
    "    b.weight = a.weight\n",
    "    b.bias = a.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc6e3969",
   "metadata": {},
   "outputs": [],
   "source": [
    "# By copying the weights and biases we can check that htey give the same values\n",
    "at = AttentionBlock(32, norm_num_groups=1)\n",
    "src = sa.q,sa.k,sa.v,sa.proj,sa.norm\n",
    "dst = at.query,at.key,at.value,at.proj_attn,at.group_norm\n",
    "for s,d in zip(src,dst): cp_parms(s,d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bfc0087",
   "metadata": {},
   "outputs": [],
   "source": [
    "rb = at(x)\n",
    "rb[0,0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a4f25e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sqkv = nn.Linear(ni, ni*3)\n",
    "st = sqkv(t)\n",
    "st.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64df0786",
   "metadata": {},
   "outputs": [],
   "source": [
    "q,k,v = torch.chunk(st, 3, dim=-1)\n",
    "q.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afdd291d",
   "metadata": {},
   "outputs": [],
   "source": [
    "(k@q.transpose(1,2)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde31928",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we just have one matrix for qkv which should be faster as it will be doing far less loading of variables \n",
    "# Different channles briing in information from different parts - this is done with multi headed attention\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, ni):\n",
    "        super().__init__()\n",
    "        self.scale = math.sqrt(ni)\n",
    "        self.norm = nn.BatchNorm2d(ni)\n",
    "        self.qkv = nn.Linear(ni, ni*3)\n",
    "        self.proj = nn.Linear(ni, ni)\n",
    "    \n",
    "    def forward(self, inp):\n",
    "        n,c,h,w = inp.shape\n",
    "        x = self.norm(inp).view(n, c, -1).transpose(1, 2)\n",
    "        q,k,v = torch.chunk(self.qkv(x), 3, dim=-1)\n",
    "        s = (q@k.transpose(1,2))/self.scale\n",
    "        x = s.softmax(dim=-1)@v\n",
    "        x = self.proj(x).transpose(1,2).reshape(n,c,h,w)\n",
    "        return x+inp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1caa223",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, ni):\n",
    "        super().__init__()\n",
    "        self.scale = math.sqrt(ni)\n",
    "        self.norm = nn.BatchNorm2d(ni)\n",
    "        self.qkv = nn.Linear(ni, ni*3)\n",
    "        self.proj = nn.Linear(ni, ni)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.norm(x).transpose(1, 2)\n",
    "        q,k,v = torch.chunk(self.qkv(x), 3, dim=-1)\n",
    "        s = (q@k.transpose(1,2))/self.scale\n",
    "        x = s.softmax(dim=-1)@v\n",
    "        return self.proj(x).transpose(1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67202ea5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 32, 16, 16])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sa = SelfAttention(32)\n",
    "sa(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eadf0c2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0047, grad_fn=<StdBackward0>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sa(x).std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a167b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heads to batch - 64 per batch and 256 pixels an \n",
    "def heads_to_batch(x, heads):\n",
    "    n,sl,d = x.shape\n",
    "    # reshape it all so that we split the last two dimensions onto heads by the rest\n",
    "    x = x.reshape(n, sl, heads, -1)\n",
    "    # if we transpose these 2 dimension it will be n by heads\n",
    "    return x.transpose(2, 1).reshape(n*heads,sl,-1)\n",
    "\n",
    "# Just  reverses it\n",
    "def batch_to_heads(x, heads):\n",
    "    n,sl,d = x.shape\n",
    "    x = x.reshape(-1, heads, sl, d)\n",
    "    return x.transpose(2, 1).reshape(-1,sl,d*heads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11734bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ein ops is inspired by einsum\n",
    "from einops import rearrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c3466d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 256, 32]), torch.Size([512, 256, 4]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is for tensor rearrangement notation turn this into this\n",
    "# so essentailly we have 3 dimensional tensor - n s (h d) where h = 8\n",
    "# now we want n * 8 and s and d the same so we are reducing teh number of d channels by 8 \n",
    "# (torch.Size([64, 256, 32]), torch.Size([512, 256, 4]))\n",
    "# now we have 4 x the number of images with teh same number of channels\n",
    "t2 = rearrange(t , 'n s (h d) -> (n h) s d', h=8)\n",
    "t.shape, t2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64105f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "t3 = rearrange(t2, '(n h) s d -> n s (h d)', h=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f34fabe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([512, 256, 4]), torch.Size([64, 256, 32]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2.shape,t3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4739d87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(t==t3).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced6c513",
   "metadata": {},
   "outputs": [],
   "source": [
    "# multi headed attention does the entire dot product on separate channels ,the heads just split out the channels\n",
    "# This allows us to have different channels to extract different information using the multi head attention\n",
    "\n",
    "class SelfAttentionMultiHead(nn.Module):\n",
    "    def __init__(self, ni, nheads):\n",
    "        super().__init__()\n",
    "        self.nheads = nheads\n",
    "        self.scale = math.sqrt(ni/nheads)\n",
    "        self.norm = nn.BatchNorm2d(ni)\n",
    "        self.qkv = nn.Linear(ni, ni*3)\n",
    "        self.proj = nn.Linear(ni, ni)\n",
    "    \n",
    "    def forward(self, inp):\n",
    "        n,c,h,w = inp.shape\n",
    "        x = self.norm(inp).view(n, c, -1).transpose(1, 2)\n",
    "        x = self.qkv(x)\n",
    "        # After teh projection they take the number of heads and they make each batch 4x bigger\n",
    "        # 1 image 32 channles - 4 images 8 channels - just make them be different images and they will have nothing to do with eachother\n",
    "        # h groups of d but now we make it n groups of h groups of 4\n",
    "        # Look above to see how they are broken down\n",
    "        x = rearrange(x, 'n s (h d) -> (n h) s d', h=self.nheads)\n",
    "        q,k,v = torch.chunk(x, 3, dim=-1)\n",
    "        s = (q@k.transpose(1,2))/self.scale\n",
    "        x = s.softmax(dim=-1)@v\n",
    "        x = rearrange(x, '(n h) s d -> n s (h d)', h=self.nheads)\n",
    "        x = self.proj(x).transpose(1,2).reshape(n,c,h,w)\n",
    "        return x+inp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ed8798",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 32, 16, 16])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sa = SelfAttentionMultiHead(32, 4)\n",
    "sx = sa(x)\n",
    "sx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c46b50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.0248, grad_fn=<MeanBackward0>),\n",
       " tensor(1.0069, grad_fn=<StdBackward0>))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sx.mean(),sx.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd9b3f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pytorch has nn.multihead attetnion - it expects teh batch to be the second dimensiion\n",
    "# if we do batch first we can make ti the same as diffisuers\n",
    "# self attention will ahve everything that will be the same - q k v projections\n",
    "# if we pass differet things we will get cross attention\n",
    "nm = nn.MultiheadAttention(32, num_heads=8, batch_first=True)\n",
    "nmx,nmw = nm(t,t,t)\n",
    "nmx = nmx + t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "451f2d42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(-0.0021, grad_fn=<MeanBackward0>),\n",
       " tensor(1.0015, grad_fn=<StdBackward0>))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "nmx.mean(), nmx.std()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
