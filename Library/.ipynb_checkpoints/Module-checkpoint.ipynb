{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5431c9fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5c0bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f272a35b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Module():\n",
    "    \n",
    "    def __call__(self, *args):\n",
    "        \"\"\" \n",
    "        Whenever our classes that inherit module are called, they should be taking their arguments\n",
    "        and carrying out the their respective implementation of the forward pass\n",
    "        \"\"\"\n",
    "        self.args = args\n",
    "        self.out = self.forward(*args)\n",
    "        return self.out\n",
    "    \n",
    "    def forward(self):\n",
    "        raise Exception(\"Forward has not been implemented.\")\n",
    "    \n",
    "    def backward(self):\n",
    "        # Call our backward function with the initial output and all arguments\n",
    "        self.bwd(self.out, *args)\n",
    "        \n",
    "    def bwd(self):\n",
    "        raise Exception(\"Backward has not been implemented\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c76daa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Linear(Module):\n",
    "    \n",
    "    def __init__(self, weight, bias):\n",
    "        self.weight = weight\n",
    "        self.bias = bias\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        inputs @ self.weight + self.bias\n",
    "    \n",
    "    def bwd(self, output, inputs):\n",
    "        # Using .t() here to be able to use matrix multiplication (@)\n",
    "        # This will allow us to transpose the input matrix\n",
    "        \n",
    "        # gradient w.r.t the inputs represents how the output of the entire linear unit affects the loss\n",
    "        inputs.g = self.output.g @ self.weight.t()\n",
    "        # gradient of the loss funtion w.r.t output * weights (since weights are the gradient of output w.r.t to weights)\n",
    "        self.weight.g = self.output.g @ self.inputs.t()\n",
    "        # gradient of L w.r.t output * 1 since gradient of output w.r.t bias is 1\n",
    "        self.bias.g = self.output.g.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1225ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Linear(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_in, n_out):\n",
    "        super().__init__()\n",
    "        self.w = torch.randn(n_in,n_out).requires_grad_()\n",
    "        self.b = torch.zeros(n_out).requires_grad_()\n",
    "        \n",
    "    def forward(self, inp): \n",
    "        return inp@self.w + self.b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffce3541",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Model(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_in, nh, n_out):\n",
    "        super().__init__()\n",
    "        self.layers = [Linear(n_in,nh), nn.ReLU(), Linear(nh,n_out)]\n",
    "        \n",
    "    def __call__(self, x, targ):\n",
    "        for l in self.layers: \n",
    "            x = l(x)\n",
    "        return F.mse_loss(x, targ[:,None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e8971d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9134d0b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
